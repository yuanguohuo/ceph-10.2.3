1. Overview
                                        +--- master-zone      : 1 or more rgw backed by ceph cluster1
                                        +--- secondary-zone-1 : 1 or more rgw backed by ceph cluster2
        +---master-zone-group ----------+ ......
        |                               +--- secondary-zone-N : 1 or more rgw backed by ceph clusterN
        |
        |                               +--- master-zone
        |                               +--- secondary-zone-1
realm---+---secondary-zone-group-1------+ ......
        |                               +--- secondary-zone-N
        +---secondary-zone-group-2
        |    
        + ......
        |    
        +---secondary-zone-group-N



2. Example
In this example, we make a simplest configuration:

                                        +--- zone_master_hyg    : client.radosgw.gateway0 on ceph-cluster-1
  realm_hyg --- zonegroup_hyg ----------+ 
                                        +--- zone_secondary_hyg : client.radosgw.gateway1 on ceph-cluster-2

2.1 Configure ceph clusters
  On node1.localdomain, configure a ceph cluster (ceph-cluster-1), without any radosgw instance;
  On node2.localdomain, configure a ceph cluster (ceph-cluster-2), without any radosgw instance;

2.2 Generate access-key and secret key:
  access-key=1WYCCJZ9JRLWZU8JTDQJ 
  secret=PXhbQDJVeF1PsXw5tsCuIaKY0N8s1BP2J3yCn9K3

2.3 Configure Master Zone
2.3.1 Create pools for master zone and delete the rbd pool
 ceph osd pool create zone_master_hyg.rgw.control 16 16
 ceph osd pool create zone_master_hyg.rgw.data.root 16 16
 ceph osd pool create zone_master_hyg.rgw.gc 16 16
 ceph osd pool create zone_master_hyg.rgw.log 16 16
 ceph osd pool create zone_master_hyg.rgw.intent-log 16 16
 ceph osd pool create zone_master_hyg.rgw.usage 16 16
 ceph osd pool create zone_master_hyg.rgw.users.keys 16 16
 ceph osd pool create zone_master_hyg.rgw.users.email 16 16
 ceph osd pool create zone_master_hyg.rgw.users.swift 16 16
 ceph osd pool create zone_master_hyg.rgw.users.uid 16 16
 ceph osd pool create zone_master_hyg.rgw.buckets.index 32 32
 ceph osd pool create zone_master_hyg.rgw.buckets.data 32 32
 ceph osd pool create zone_master_hyg.rgw.meta 16 16
 rados rmpool rbd rbd --yes-i-really-really-mean-it

2.3.2 Create the realm
  radosgw-admin realm create --rgw-realm=realm_hyg --default

2.3.3 Create the master zone group
  radosgw-admin zonegroup create --rgw-zonegroup=zonegroup_hyg --endpoints=http://node1.localdomain:8000 --master --default

2.3.4 Create the master zone
  radosgw-admin zone create --rgw-zonegroup=zonegroup_hyg --rgw-zone=zone_master_hyg --endpoints=http://node1.localdomain:8000 --access-key=1WYCCJZ9JRLWZU8JTDQJ --secret=PXhbQDJVeF1PsXw5tsCuIaKY0N8s1BP2J3yCn9K3 --default --master

2.3.5 Create the user with the generated access and secret key
  radosgw-admin user create --uid=yuanguo --display-name="Yuanguo Huo"  --access-key=1WYCCJZ9JRLWZU8JTDQJ --secret=PXhbQDJVeF1PsXw5tsCuIaKY0N8s1BP2J3yCn9K3 --system

2.3.6 Update the period
  radosgw-admin period update --commit

2.3.7 Edit the ceph.conf by adding these lines
  [client.radosgw.gateway0]
    rgw_frontends = "civetweb port=8000"
    rgw_zone=zone_master_hyg
    admin_socket = /var/ceph/sock/gateway0.asock
    rgw socket path = /var/ceph/sock/gateway0.sock
    keyring = /var/ceph/rgwkeyr/gateway0.keyring
    log file = /var/ceph/logs/gateway0.log
    debug_rgw = 100
    debug_objecter = 100
    rgw_override_bucket_index_max_shards = 6
    rgw_md_log_max_shards = 4
    rgw_num_zone_opstate_shards = 8
    rgw_data_log_num_shards = 8
    rgw_objexp_hints_num_shards = 7

2.3.8 Start the rgw
  systemctl start ceph-radosgw@0


2.4 Configure Secondary Zone
2.4.1 Create pools for secondary zone and delete the rbd pool
 ceph osd pool create zone_secondary_hyg.rgw.control 16 16
 ceph osd pool create zone_secondary_hyg.rgw.data.root 16 16
 ceph osd pool create zone_secondary_hyg.rgw.gc 16 16
 ceph osd pool create zone_secondary_hyg.rgw.log 16 16
 ceph osd pool create zone_secondary_hyg.rgw.intent-log 16 16
 ceph osd pool create zone_secondary_hyg.rgw.usage 16 16
 ceph osd pool create zone_secondary_hyg.rgw.users.keys 16 16
 ceph osd pool create zone_secondary_hyg.rgw.users.email 16 16
 ceph osd pool create zone_secondary_hyg.rgw.users.swift 16 16
 ceph osd pool create zone_secondary_hyg.rgw.users.uid 16 16
 ceph osd pool create zone_secondary_hyg.rgw.buckets.index 32 32
 ceph osd pool create zone_secondary_hyg.rgw.buckets.data 32 32
 ceph osd pool create zone_secondary_hyg.rgw.meta 16 16
 rados rmpool rbd rbd --yes-i-really-really-mean-it

2.4.2 Pull the realm
  radosgw-admin realm pull --url=http://node1.localdomain:8000 --access-key=1WYCCJZ9JRLWZU8JTDQJ --secret=PXhbQDJVeF1PsXw5tsCuIaKY0N8s1BP2J3yCn9K3

2.4.3 Pull the period
  radosgw-admin period pull --url=http://node1.localdomain:8000 --access-key=1WYCCJZ9JRLWZU8JTDQJ --secret=PXhbQDJVeF1PsXw5tsCuIaKY0N8s1BP2J3yCn9K3

2.4.4 Set the real and zonegroup just pulled as default
  radosgw-admin realm default --rgw-realm=realm_hyg
  radosgw-admin zonegroup default --rgw-zonegroup=zonegroup_hyg

2.4.5 Create the secondary zone 
  Notice: maste side and secondary side share the same realm and zonegroup, so realm and zonegroup are pulled from master side, but zones are different, so create it:
  radosgw-admin zone create --rgw-zonegroup=zonegroup_hyg --rgw-zone=zone_secondary_hyg --endpoints=http://node2.localdomain:8000 --default --access-key=1WYCCJZ9JRLWZU8JTDQJ --secret=PXhbQDJVeF1PsXw5tsCuIaKY0N8s1BP2J3yCn9K3

2.4.6 Update the peroid
  radosgw-admin period update --commit --rgw-zone=zone_secondary_hyg

2.4.7 Edit the ceph.conf by adding these lines
  [client.radosgw.gateway1]
    rgw_frontends = "civetweb port=8000"
    rgw_zone=zone_secondary_hyg
    admin_socket = /var/ceph/sock/gateway1.asock
    rgw socket path = /var/ceph/sock/gateway1.sock
    keyring = /var/ceph/rgwkeyr/gateway1.keyring
    log file = /var/ceph/logs/gateway1.log
    debug_rgw = 100
    debug_objecter = 100
    rgw_override_bucket_index_max_shards = 6
    rgw_md_log_max_shards = 4
    rgw_num_zone_opstate_shards = 8
    rgw_data_log_num_shards = 8
    rgw_objexp_hints_num_shards = 7


2.4.8 Start the rgw
  systemctl start ceph-radosgw@1


2.5 Check sync status
  radosgw-admin sync status




==============================================================================================
             Pools and Info in them                        
==============================================================================================
                        
3. Pools and Info in them                        

3.1 Pool {zone}.rgw.data.root
# rados ls -p zone_master_hyg.rgw.data.root
testbuckBBB
testbuckCCC
.bucket.meta.testbuckAAA:c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1  <-- bucket-instance-info
.bucket.meta.testbuckBBB:c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2
testbuckAAA 
.bucket.meta.testbuckCCC:c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3


3.1.1 oid of bucket-instance-info
    oid           =  ".bucket.meta."  + {bucket-name} + ":" + {bucket-marker}
    bucket-marker =  {zone-id} + ...

    for example: .bucket.meta.testbuckAAA:c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1
          bucket-name   : testbuckAAA
          bucket-marker : c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1
          zone-id       : c6a39056-1ff3-408a-aea6-80bfc9dc9709


3.1.2 omap of testbuckAAA
  no omap 

3.1.3 xattrs of testbuckAAA:
  ceph.objclass.version

3.1.4 content of testbuckAAA
  unreadable, but there are readable chars:
      bucket name
      owner
      marker(c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1)
      data pool(zone_master_hyg.rgw.buckets.data)
      index pool(zone_master_hyg.rgw.buckets.index)
      data extra pool(zone_master_hyg.rgw.buckets.non-ec)

3.1.5 omap of bucket-instance-info
  no omap 

3.1.6 xattrs of bucket-instance-info
ceph.objclass.version
user.rgw.acl

3.1.7 content of bucket-instance-info 
  unreadable, but there are readable chars:
      bucket name,
      owner,
      marker,
      data pool,
      index pool,
      data extra pool,
      placement,
      num_shards of the bucket (known from code)

  In content of bucket-instance-info, there are 2 important members:
    1. the bucket marker; such as c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1;
    2. num_shards of the bucket. It can be configured by rgw_override_bucket_index_max_shards (it's 6 in my example);

  When an obj is written into a bucket, it's sharded into one of the bucket-shards, by the obj name. for, example, I write 
  6 objects (OBJ_111 ~ OBJ_666) to testbuckAAA and testbuckBBB respectively;
    a. in testbuckAAA: OBJ_444 and OBJ_555 are sharded into shard-4 of testbuckAAA;
    b. in testbuckBBB: OBJ_444 and OBJ_555 are sharded into shard-4 of testbuckBBB;
  This means that, given a bucket, objects are sharded into one of its shards by obj name, i.e.
    shard_id = hash(obj name) % num_shards
    
  Once we know the bucket marker and the shard_id, we know the bucket-shard-instance oid (in pool{zone}.rgw.buckets.index):
        oid = ".dir" + "." + bucket_marker + "." + shard_id
  Let's see pool {zone}.rgw.buckets.index

3.2 Pool {zone}.rgw.buckets.index
   #rados ls  -p  zone_master_hyg.rgw.buckets.index | sort
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.0   ------------
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.1          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.2     6 shards of testbuckAAA
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.3          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.4          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.5   ------------
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2.0   ------------
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2.1            |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2.2     6 shards of testbuckBBB
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2.3          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2.4          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.2.5   ------------
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3.0   ------------
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3.1          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3.2     6 shards of testbuckCCC
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3.3          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3.4          |
  .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.3.5   -------------

3.2.1 content of bucket-shard-instance
  no content

3.2.2 xattrs of bucket-shard-instance
  no xattrs

3.2.3 omap of bucket-shard-instance

  As seen above, when an obj is written to a bucket, the obj is sharded into one of the bucket's bucket-shard. 
  For example:
    When OBJ_444 is written to testbuckAAA (marker: c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1), it's sharded into shard-4,
    so we know the bucket-shard-instance oid = .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.4;
    Then, some info the obj testbuckAAA/OBJ_444 is recorded in omap of this bucket-shard-instance, as shown below:
          OBJ_444
          .0_00000000001.4.2 
          .0_00000000002.5.3
                
    When OBJ_555 is written to testbuckAAA, it's sharded into the same shard, 
    thus, the info of obj testbuckAAA/OBJ_555 is recorded in the omap of the same bucket-shard-instance:
          OBJ_555
          .0_00000000003.6.2
          .0_00000000004.7.3

    See in in detail:
      # rados listomapvals .dir.c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1.4 -p zone_master_hyg.rgw.buckets.index
      OBJ_444
      value (214 bytes) :
      00000000  08 03 d0 00 00 00 07 00  00 00 4f 42 4a 5f 34 34  |..........OBJ_44|
      00000010  34 01 00 00 00 00 00 00  00 01 04 03 65 00 00 00  |4...........e...|
      00000020  01 fa 12 00 00 00 00 00  00 db 47 cf 58 9d 80 f0  |..........G.X...|
      00000030  31 20 00 00 00 64 36 33  30 30 30 64 36 63 37 64  |1 ...d63000d6c7d|
      00000040  32 37 61 31 36 39 66 62  37 66 64 31 34 34 62 64  |27a169fb7fd144bd|
      00000050  37 66 65 35 65 07 00 00  00 79 75 61 6e 67 75 6f  |7fe5e....yuanguo|
      00000060  0b 00 00 00 59 75 61 6e  67 75 6f 20 48 75 6f 0a  |....Yuanguo Huo.|
      00000070  00 00 00 74 65 78 74 2f  70 6c 61 69 6e fa 12 00  |...text/plain...|
      00000080  00 00 00 00 00 00 00 00  00 00 00 00 00 01 01 02  |................|
      00000090  00 00 00 0c 01 02 2e 00  00 00 63 36 61 33 39 30  |..........c6a390|
      000000a0  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
      000000b0  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
      000000c0  31 32 32 2e 32 32 30 31  00 00 00 00 00 00 00 00  |122.2201........|
      000000d0  00 00 00 00 00 00                                 |......|
      000000d6
      
      OBJ_555
      value (214 bytes) :
      00000000  08 03 d0 00 00 00 07 00  00 00 4f 42 4a 5f 35 35  |..........OBJ_55|
      00000010  35 01 00 00 00 00 00 00  00 01 04 03 65 00 00 00  |5...........e...|
      00000020  01 fa 12 00 00 00 00 00  00 e0 47 cf 58 ba 36 71  |..........G.X.6q|
      00000030  24 20 00 00 00 64 36 33  30 30 30 64 36 63 37 64  |$ ...d63000d6c7d|
      00000040  32 37 61 31 36 39 66 62  37 66 64 31 34 34 62 64  |27a169fb7fd144bd|
      00000050  37 66 65 35 65 07 00 00  00 79 75 61 6e 67 75 6f  |7fe5e....yuanguo|
      00000060  0b 00 00 00 59 75 61 6e  67 75 6f 20 48 75 6f 0a  |....Yuanguo Huo.|
      00000070  00 00 00 74 65 78 74 2f  70 6c 61 69 6e fa 12 00  |...text/plain...|
      00000080  00 00 00 00 00 00 00 00  00 00 00 00 00 01 01 02  |................|
      00000090  00 00 00 0c 01 04 2e 00  00 00 63 36 61 33 39 30  |..........c6a390|
      000000a0  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
      000000b0  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
      000000c0  31 32 32 2e 32 32 31 34  00 00 00 00 00 00 00 00  |122.2214........|
      000000d0  00 00 00 00 00 00                                 |......|
      000000d6
      
      key (18 bytes):
      00000000  80 30 5f 30 30 30 30 30  30 30 30 30 30 31 2e 34  |.0_00000000001.4|
      00000010  2e 32                                             |.2|
      00000012
      
      value (127 bytes) :
      00000000  03 01 79 00 00 00 0f 00  00 00 30 30 30 30 30 30  |..y.......000000|
      00000010  30 30 30 30 31 2e 34 2e  32 07 00 00 00 4f 42 4a  |00001.4.2....OBJ|
      00000020  5f 34 34 34 00 00 00 00  00 00 00 00 01 01 0a 00  |_444............|
      00000030  00 00 88 ff ff ff ff ff  ff ff ff 00 2e 00 00 00  |................|
      00000040  63 36 61 33 39 30 35 36  2d 31 66 66 33 2d 34 30  |c6a39056-1ff3-40|
      00000050  38 61 2d 61 65 61 36 2d  38 30 62 66 63 39 64 63  |8a-aea6-80bfc9dc|
      00000060  39 37 30 39 2e 34 31 32  32 2e 32 32 30 31 00 00  |9709.4122.2201..|
      00000070  01 00 00 00 00 00 00 00  00 00 00 00 00 00 00     |...............|
      0000007f
      
      key (18 bytes):
      00000000  80 30 5f 30 30 30 30 30  30 30 30 30 30 32 2e 35  |.0_00000000002.5|
      00000010  2e 33                                             |.3|
      00000012
      
      value (119 bytes) :
      00000000  03 01 71 00 00 00 0f 00  00 00 30 30 30 30 30 30  |..q.......000000|
      00000010  30 30 30 30 32 2e 35 2e  33 07 00 00 00 4f 42 4a  |00002.5.3....OBJ|
      00000020  5f 34 34 34 db 47 cf 58  9d 80 f0 31 01 01 02 00  |_444.G.X...1....|
      00000030  00 00 0c 01 2e 00 00 00  63 36 61 33 39 30 35 36  |........c6a39056|
      00000040  2d 31 66 66 33 2d 34 30  38 61 2d 61 65 61 36 2d  |-1ff3-408a-aea6-|
      00000050  38 30 62 66 63 39 64 63  39 37 30 39 2e 34 31 32  |80bfc9dc9709.412|
      00000060  32 2e 32 32 30 31 00 01  02 00 00 00 00 00 00 00  |2.2201..........|
      00000070  00 00 00 00 00 00 00                              |.......|
      00000077
      
      key (18 bytes):
      00000000  80 30 5f 30 30 30 30 30  30 30 30 30 30 33 2e 36  |.0_00000000003.6|
      00000010  2e 32                                             |.2|
      00000012
      
      value (127 bytes) :
      00000000  03 01 79 00 00 00 0f 00  00 00 30 30 30 30 30 30  |..y.......000000|
      00000010  30 30 30 30 33 2e 36 2e  32 07 00 00 00 4f 42 4a  |00003.6.2....OBJ|
      00000020  5f 35 35 35 00 00 00 00  00 00 00 00 01 01 0a 00  |_555............|
      00000030  00 00 88 ff ff ff ff ff  ff ff ff 00 2e 00 00 00  |................|
      00000040  63 36 61 33 39 30 35 36  2d 31 66 66 33 2d 34 30  |c6a39056-1ff3-40|
      00000050  38 61 2d 61 65 61 36 2d  38 30 62 66 63 39 64 63  |8a-aea6-80bfc9dc|
      00000060  39 37 30 39 2e 34 31 32  32 2e 32 32 31 34 00 00  |9709.4122.2214..|
      00000070  03 00 00 00 00 00 00 00  00 00 00 00 00 00 00     |...............|
      0000007f
      
      key (18 bytes):
      00000000  80 30 5f 30 30 30 30 30  30 30 30 30 30 34 2e 37  |.0_00000000004.7|
      00000010  2e 33                                             |.3|
      00000012
      
      value (119 bytes) :
      00000000  03 01 71 00 00 00 0f 00  00 00 30 30 30 30 30 30  |..q.......000000|
      00000010  30 30 30 30 34 2e 37 2e  33 07 00 00 00 4f 42 4a  |00004.7.3....OBJ|
      00000020  5f 35 35 35 e0 47 cf 58  ba 36 71 24 01 01 02 00  |_555.G.X.6q$....|
      00000030  00 00 0c 01 2e 00 00 00  63 36 61 33 39 30 35 36  |........c6a39056|
      00000040  2d 31 66 66 33 2d 34 30  38 61 2d 61 65 61 36 2d  |-1ff3-408a-aea6-|
      00000050  38 30 62 66 63 39 64 63  39 37 30 39 2e 34 31 32  |80bfc9dc9709.412|
      00000060  32 2e 32 32 31 34 00 01  04 00 00 00 00 00 00 00  |2.2214..........|
      00000070  00 00 00 00 00 00 00                              |.......|
      00000077


3.3 Pool {zone}.rgw.log

  # rados ls -p zone_master_hyg.rgw.log  | sort
  bucket.sync-status.36c78219-6be2-4f58-936c-a975d16d6e93:testbuckAAA:c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1:0
  bucket.sync-status.36c78219-6be2-4f58-936c-a975d16d6e93:testbuckAAA:c6a39056-1ff3-408a-aea6-80bfc9dc9709.4122.1:1
  ......
  data.full-sync.index.36c78219-6be2-4f58-936c-a975d16d6e93.0
  data.full-sync.index.36c78219-6be2-4f58-936c-a975d16d6e93.1
  ......
  data_log.0
  data_log.4
  data_log.5
  data_log.6
  data_log.7
  datalog.sync-status.36c78219-6be2-4f58-936c-a975d16d6e93
  datalog.sync-status.shard.36c78219-6be2-4f58-936c-a975d16d6e93.0
  ......
  meta.log.455f94da-53d2-4fec-9086-3711cd24d97c.17
  meta.log.5311385b-3479-47c8-aeb8-26c08ca6c5cc.1
  obj_delete_at_hint.0000000000
  obj_delete_at_hint.0000000001
  ......
  sync.error-log.1
  sync.error-log.10
  sync.error-log.11
  ......

3.3.1 content of data_log.X
  no content

3.3.2 xattrs of data_log.X
  no xattrs 


3.3.3 omap of data_log.X

  The objects written into the zone are sharded into N=rgw_data_log_num_shards (8 in my 
  example) shards and logs are saved in omap of:
        data_log.0
        data_log.1
        ......
        data_log.N-1
  
  For example, I have written 6 files to testbuckAAA, 6 files to testbuckBBB. AS we see 
  below, there are 12 logs in  (there is no data_log.1, data_log.2 or data_log.0 because
  no objects are sharded into those shards, I have written merely 12 objects)
        data_log.0
        data_log.4
        data_log.5
        data_log.6
        data_log.7

  The sharding is based on bucket name and bucket shard_id. Notice that bucket shard_id is 
  based on obj name, so, the log sharding is based on bucket name and obj name. i.e.
        log_shard_id = hash(bucket-name, shard_id)
        shard_id = hash(obj name) % num_shards    <---- see section 3.1.7

  See in in detail:

  # for xx in `rados ls -p zone_master_hyg.rgw.log | sort | grep data_log` ; do  echo "========== omap key-val paris of $xx ==========";  rados listomapvals $xx -p zone_master_hyg.rgw.log  ; done > tmp.hyg

  # cat tmp.hyg
  ========== omap key-val paris of data_log.0 ==========
  1_1489979355.933721_12.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 31 3a 34 db  47 cf 58 f6 75 a7 37 4c  |122.1:4.G.X.u.7L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 31 3a 34 db  47 cf 58 f6 75 a7 37 18  |122.1:4.G.X.u.7.|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 35 35 2e  |...1_1489979355.|
  000000b0  39 33 33 37 32 31 5f 31  32 2e 31                 |933721_12.1|
  000000bb
  
  1_1489979365.664604_21.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 31 3a 34 e5  47 cf 58 17 10 9d 27 4c  |122.1:4.G.X...'L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 31 3a 34 e5  47 cf 58 17 10 9d 27 18  |122.1:4.G.X...'.|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 36 35 2e  |...1_1489979365.|
  000000b0  36 36 34 36 30 34 5f 32  31 2e 31                 |664604_21.1|
  000000bb
  
  1_1489979397.374156_23.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 32 3a 34 05  48 cf 58 32 2b 4d 16 4c  |122.2:4.H.X2+M.L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 32 3a 34 05  48 cf 58 32 2b 4d 16 18  |122.2:4.H.X2+M..|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 39 37 2e  |...1_1489979397.|
  000000b0  33 37 34 31 35 36 5f 32  33 2e 31                 |374156_23.1|
  000000bb
  
  1_1489979409.713300_24.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 32 3a 34 11  48 cf 58 74 1a 84 2a 4c  |122.2:4.H.Xt..*L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 32 3a 34 11  48 cf 58 74 1a 84 2a 18  |122.2:4.H.Xt..*.|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 34 30 39 2e  |...1_1489979409.|
  000000b0  37 31 33 33 30 30 5f 32  34 2e 31                 |713300_24.1|
  000000bb
  
  ========== omap key-val paris of data_log.4 ==========
  1_1489979342.100539_83.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 31 3a 30 ce  47 cf 58 6f 1b fe 05 4c  |122.1:0.G.Xo...L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 31 3a 30 ce  47 cf 58 6f 1b fe 05 18  |122.1:0.G.Xo....|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 34 32 2e  |...1_1489979342.|
  000000b0  31 30 30 35 33 39 5f 38  33 2e 31                 |100539_83.1|
  000000bb
  
  1_1489979380.555422_87.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 32 3a 30 f4  47 cf 58 14 12 1b 21 4c  |122.2:0.G.X...!L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 32 3a 30 f4  47 cf 58 14 12 1b 21 18  |122.2:0.G.X...!.|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 38 30 2e  |...1_1489979380.|
  000000b0  35 35 35 34 32 32 5f 38  37 2e 31                 |555422_87.1|
  000000bb
  
  ========== omap key-val paris of data_log.5 ==========
  1_1489979370.122105_60.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 31 3a 31 ea  47 cf 58 d7 2c 47 07 4c  |122.1:1.G.X.,G.L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 31 3a 31 ea  47 cf 58 d7 2c 47 07 18  |122.1:1.G.X.,G..|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 37 30 2e  |...1_1489979370.|
  000000b0  31 32 32 31 30 35 5f 36  30 2e 31                 |122105_60.1|
  000000bb
  
  1_1489979411.194656_64.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 32 3a 31 13  48 cf 58 ac 37 9a 0b 4c  |122.2:1.H.X.7..L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 32 3a 31 13  48 cf 58 ac 37 9a 0b 18  |122.2:1.H.X.7...|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 34 31 31 2e  |...1_1489979411.|
  000000b0  31 39 34 36 35 36 5f 36  34 2e 31                 |194656_64.1|
  000000bb
  
  ========== omap key-val paris of data_log.6 ==========
  1_1489979347.602041_59.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 31 3a 32 d3  47 cf 58 49 6e e2 23 4c  |122.1:2.G.XIn.#L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 31 3a 32 d3  47 cf 58 49 6e e2 23 18  |122.1:2.G.XIn.#.|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 34 37 2e  |...1_1489979347.|
  000000b0  36 30 32 30 34 31 5f 35  39 2e 31                 |602041_59.1|
  000000bb
  
  1_1489979387.079777_65.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 32 3a 32 fb  47 cf 58 00 4e c1 04 4c  |122.2:2.G.X.N..L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 32 3a 32 fb  47 cf 58 00 4e c1 04 18  |122.2:2.G.X.N...|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 38 37 2e  |...1_1489979387.|
  000000b0  30 37 39 37 37 37 5f 36  35 2e 31                 |079777_65.1|
  000000bb
  
  ========== omap key-val paris of data_log.7 ==========
  1_1489979351.693028_85.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 31 3a 33 d7  47 cf 58 81 c7 4e 29 4c  |122.1:3.G.X..N)L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 41 41  41 3a 63 36 61 33 39 30  |stbuckAAA:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 31 3a 33 d7  47 cf 58 81 c7 4e 29 18  |122.1:3.G.X..N).|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 35 31 2e  |...1_1489979351.|
  000000b0  36 39 33 30 32 38 5f 38  35 2e 31                 |693028_85.1|
  000000bb
  
  1_1489979392.541559_89.1
  value (187 bytes) :
  00000000  02 01 b5 00 00 00 00 00  00 00 39 00 00 00 74 65  |..........9...te|
  00000010  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000020  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000030  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000040  31 32 32 2e 32 3a 33 00  48 cf 58 9c 8a 47 20 4c  |122.2:3.H.X..G L|
  00000050  00 00 00 01 01 46 00 00  00 01 39 00 00 00 74 65  |.....F....9...te|
  00000060  73 74 62 75 63 6b 42 42  42 3a 63 36 61 33 39 30  |stbuckBBB:c6a390|
  00000070  35 36 2d 31 66 66 33 2d  34 30 38 61 2d 61 65 61  |56-1ff3-408a-aea|
  00000080  36 2d 38 30 62 66 63 39  64 63 39 37 30 39 2e 34  |6-80bfc9dc9709.4|
  00000090  31 32 32 2e 32 3a 33 00  48 cf 58 9c 8a 47 20 18  |122.2:3.H.X..G .|
  000000a0  00 00 00 31 5f 31 34 38  39 39 37 39 33 39 32 2e  |...1_1489979392.|
  000000b0  35 34 31 35 35 39 5f 38  39 2e 31                 |541559_89.1|
  000000bb




====================
data-log-shard:  
bucket-shard  :

RGWDataSyncCR:
1.  RGWReadDataSyncStatusCoroutine

  1.1 read local ceph cluster (RGWReadDataSyncStatusCoroutine)
            pool: {zone}.rgw.log    
            obj: datalog.sync-status.{remote-zone-id}
      to get 'state' and 'num_shards', and save into
      sync_status.sync_info (see parent class RGWSimpleRadosReadCR)

  1.2 for each data-log-shard X, read the content of following object in local ceph cluster, 
            pool: {zone}.rgw.log
            obj: datalog.sync-status.shard.{remote-zone-id}.X
      to get state(full or increment sync), marker, next_step_marker,
      timestamp ... and save into sync_status->sync_markers[X]


2 (if 'state' got in 1.1 is StateInit), RGWInitDataSyncStatusCoroutine

  2.1  lock datalog.sync-status.{remote-zone-id}  (the same obj in step 1.1)
  2.2  recreate datalog.sync-status.{remote-zone-id}  
  2.3  lock it again

  2.4  for each data-log-shard X, read remote ceph cluster
            pool: {remote-zone}.rgw.log 
            obj:  data_log.X
       and save into shards_info[X]

  2.5  for each data-log-shard X, save shards_info[X] to local ceph cluster,
            pool: {zone}.rgw.log 
            obj: datalog.sync-status.shard.{remote-zone-id}.X
       Notice: See step 1.2, they are the same objs.

  2.6  change  sync_status.sync_info.state to StateBuildingFullSyncMaps, and
       save into local ceph cluster:
            pool: {zone}.rgw.log
            obj: datalog.sync-status.{remote-zone-id}
       Notice: See step 1.1, it's the same obj. Now, we are changing the state.

  2.7  unlock datalog.sync-status.{remote-zone-id}  (the same obj in step 1.1, 2.1)

3 (if 'state' got in 1.1 is StateBuildingFullSyncMaps), RGWListBucketIndexesCR 


4 (if 'state' got in 1.1 is StateSync),  for each data-log-shard X, 
         RGWDataSyncShardControlCR -->
         RGWDataSyncShardCR -->
   
  4.1 RGWDataSyncShardCR::incremental_sync
  
  4.1.1 create an error repository, error_repo: datalog.sync-status.shard.{remote-zone-id}.X.retry
        error_repo->append(key) is to set an omap KV pair (key=key, val=empty)
        failed bucket shards will be appended here, and will be processed (retried) at 4.1.5
  4.1.2 lock "datalog.sync-status.shard.{remote-zone-id}.X" for a period of lock_duration
  4.1.3 create a marker_tracker;  it's used to 
         i.  check if a bucket shard is in progress. See 4.1.9;  2.
         ii. update the maker.  See 4.1.7  and 4.1.9.C
  4.1.4 process out of band updates (Yuanguo: generated by notification ??)
  4.1.5 process bucket shards that previously failed.
  4.1.6 read omap header of the following object from remote zone
              pool: {zone}.rgw.log
              obj : data_log.X
        The map header contains:
              string max_marker;
              utime_t max_time;
  4.1.7 if the data log marker got in 4.1.6 is NOT newer than what was recorded locally,
        this round of incremental sync is finished.
        See 1.2, where the recorded markers were read from local ceph cluster. 
               pool: {zone}.rgw.log
               obj: datalog.sync-status.shard.{remote-zone-id}.X
               Yuanguo: where did these markers get updated ??? 
                  Answer: the marker_tracker, See 4.1.3 and 4.1.9.C
  4.1.8 else, if the data log marker got in 4.1.6 is newer than what was recorded locally
        read omap KV paris of the following object from remote zone
              pool: {zone}.rgw.log
              obj : data_log.X
        What are the KV pairs? See RGWDataChangesLog::add_entry. They are:
              1_1489653363.684562_381.1 => some info including bucke-tname, bucked-id, bucket-shard, modification timestamp
              Notice the key of KV pair is not 'change.key', but generated at: cls/log/cls_log.cc:cls_log_add
        The longer story: we have B buckets, each has rgw_override_bucket_index_max_shards shards, then
                hash(bucket-N, S) = D     the hash is function choose_oid()
                    S is the 'bucket-shard', D is the 'data-log-shard', and
                    0 <= N < B  <------ all buckets
                    0 <= S < rgw_override_bucket_index_max_shards  <------ all shards of all buckets
                    0 <= D < rgw_data_log_num_shards  <------- all shards of all buckets are sharded into [0, rgw_data_log_num_shards)
              that is to say, any modification of a bucket will be recorded as omap KV pair of exactly one "data_log.[0-rgw_data_log_num_shards)"
              we are considering the specific data_log.X now, and we may get something like: 
                    1_1490768507.881727_3.1 => bucket-2 shard3 timestamp1
                    1_1490768519.257779_4.1 => bucket-5 shard1 timestamp2
                    1_1490768527.759371_5.1 => bucket-1 shard4 timestamp3
                    1_1490768541.378290_6.1 => bucket-6 shard0 timestamp4
                    ......
                          
  4.1.9 For any entry returned in 4.1.8 (it's some info like: 1_1490768519.257779_4.1 => bucket-5 shard1 timestamp2)
        if it's not already in progress (checked by the marker_tracker created in 4.1.3) or not duplicated, 
        handle it by RGWDataSyncSingleEntryCR.
           A. parse the entry to get bucket-name:bucket-5, bucket-shard:shard1,
           B. call RGWRunBucketSyncCoroutine
               a. get xattrs from local ceph cluster (RGWReadBucketSyncStatusCoroutine)
                       pool: {zone}.rgw.log 
                       obj: bucket.sync-status.{remote-zone-id}:{bucketname}:{bucket_id}:{shard_id}
                  the xattrs include:
                       full_marker
                       inc_marker
                       lock.sync_lock
                       lock.sync_lock.incremental
                       state
               b. get bucket instance info from local ceph cluster, if not
                  present ...
               c. if state got in step a is StateInit, 
                      call RGWInitBucketShardSyncStatusCoroutine
                            lock obj=bucket.sync-status.{remote-zone-id}:{bucketname}:{bucket_id}:{shard_id}
                            recreate that obj
                            lock obj again;
                            read bucket index log info from remote zone;
                            write what's read above in local ceph cluster;
                            unlock obj
                      set state as StateFullSync (do full sync next time)
               b. if state got in step a is StateFullSync, 
                      call RGWBucketShardFullSyncCR
                      set state as StateIncrementalSync (do incremental sync next time)
               d. if state got in step a is StateIncrementalSync,
                      call RGWBucketShardIncrementalSyncCR
           C. marker_tracker->finish ---> ... ---> marker_tracker->store_marker. 
              Update new marker as content of the following object in local ceph cluster (See 4.1.3 and 4.1.7)
                   pool: {zone}.rgw.log
                   obj: datalog.sync-status.shard.{remote-zone-id}.X
